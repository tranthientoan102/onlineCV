{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student name: Thien Toan Tran <br> Student ID: A1808080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assignment 3 Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/T10I4D100K.dat',\n",
       " './data/T40I10D100K.dat',\n",
       " './data/chess.dat',\n",
       " './data/connect.dat',\n",
       " './data/mushroom.dat',\n",
       " './data/pumsb.dat',\n",
       " './data/pumsb_star.dat']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath = './data/'\n",
    "fileDataPath = []\n",
    "fileDataNamesOnly = []\n",
    "for a in os.listdir(dataPath):\n",
    "    if a.endswith('.dat'):\n",
    "        fileDataPath.append(dataPath + a)\n",
    "        fileDataNamesOnly.append(a.split('.')[0])\n",
    "\n",
    "fileDataPath = sorted(fileDataPath)\n",
    "fileDataNamesOnly = sorted(fileDataNamesOnly)\n",
    "fileDataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./data/T10I4D100K.dat': 100000,\n",
       " './data/T40I10D100K.dat': 100000,\n",
       " './data/chess.dat': 3196,\n",
       " './data/connect.dat': 67557,\n",
       " './data/mushroom.dat': 8124,\n",
       " './data/pumsb.dat': 49046,\n",
       " './data/pumsb_star.dat': 49046}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLineCounts = dict()\n",
    "for path in fileDataPath:\n",
    "    dataLineCounts[path] = sum(1 for line in open(path))\n",
    "dataLineCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 1: Frequent Itemsets\n",
    "#### 1.1. Implement the simple, randomized algorithm given in 6.4.1\n",
    "\n",
    "Suppose need to randomly select n out of m baskets in the entire file, where $\\frac{n}{m} = p$\n",
    "\n",
    "My random algorithm will be implemented as follow:\n",
    "1. select random subset of entire dataset\n",
    "    * For each of basket in data file, a random float in the range [0.0, 1.0) will be generated. It will represent the selection probability for this basket.\n",
    "    * If the probability $\\geq 1- p$, then this basket will be selected for further process\n",
    "1. Full itemset is initialized as a Python dictionary, where key is an itemset, value is its count on the subset\n",
    "1. For each basket data, generate a set of itemset and update the full itemset\n",
    "    * since the singleton and pairs are likely to dominate, only those 2 types are generated\n",
    "    * for each generated itemset,\n",
    "        * if it exists in the full itemset dictionary, then its value is increased by 1\n",
    "        * otherwise, create new record using itself as key and value is 1\n",
    "1. count itemset over the subset\n",
    "1. sort itemsets and get most frequent itemset in the subset\n",
    "    * lower the confident to $ps$, since the original data has expected confident $s$ and we only process a portion $p$ of the data\n",
    "    * lower the confident more, like $0.9ps$, if there is enough memory. By doing that, the result is more likely to achieve the confident $s$ in the whole dataset\n",
    "    * equivalently change to least support count $0.9psm$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def randomSelect(m,p):\n",
    "    \"\"\"\n",
    "    Randomly select ids based on given probability\n",
    "\n",
    "    :param m: dataset's record\n",
    "    :param p: probability\n",
    "    :return: list of selected ids\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(m):\n",
    "        if random.random().__ge__(1-p):\n",
    "            result.append(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Test random select result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=100, p=0.5: expected: 50.0, result: 51, delta = -2.0%\n",
      "m=100, p=0.8: expected: 80.0, result: 71, delta = 11.25%\n",
      "m=100, p=0.3: expected: 30.0, result: 31, delta = -3.3333333333333335%\n",
      "\n",
      "m=500, p=0.5: expected: 250.0, result: 268, delta = -7.199999999999999%\n",
      "m=500, p=0.8: expected: 400.0, result: 395, delta = 1.25%\n",
      "m=500, p=0.3: expected: 150.0, result: 154, delta = -2.666666666666667%\n",
      "\n",
      "m=1000, p=0.5: expected: 500.0, result: 486, delta = 2.8000000000000003%\n",
      "m=1000, p=0.8: expected: 800.0, result: 790, delta = 1.25%\n",
      "m=1000, p=0.3: expected: 300.0, result: 302, delta = -0.6666666666666667%\n",
      "\n",
      "m=10000, p=0.5: expected: 5000.0, result: 4984, delta = 0.32%\n",
      "m=10000, p=0.8: expected: 8000.0, result: 7947, delta = 0.6625%\n",
      "m=10000, p=0.3: expected: 3000.0, result: 3017, delta = -0.5666666666666667%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_m = [100,500,1000,10000]\n",
    "list_p = [0.5,0.8,0.3]\n",
    "for m in list_m:\n",
    "    for p in list_p:\n",
    "        a = randomSelect(m, p)\n",
    "        expect = m*p\n",
    "        selected = len(a)\n",
    "        pc = (expect- selected)/expect\n",
    "        print(f'm={m}, p={p}: expected: {expect}, result: {selected}, delta = {pc*100}%')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def testBasketIsSorted(basket: str):\n",
    "#     item = basket.split('\\\\s')\n",
    "#     i = 0\n",
    "#     while i < len(item)-2:\n",
    "#         if item[i]>= item[i+1]: return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generateItemset(basketItem:[int]):\n",
    "    \"\"\"\n",
    "    Generate singleton and pairs from basket's item id\n",
    "\n",
    "    :param basketItem: array of basket's item id\n",
    "    :return: set of Itmeset\n",
    "    \"\"\"\n",
    "    result = set()\n",
    "    for i in range(1, 4):\n",
    "        tmpset = combinations(basketItem,i)\n",
    "        for j in tmpset: result.add(j)\n",
    "\n",
    "    # i = 0\n",
    "    # while i < len(basketItem):\n",
    "    #     result.add((basketItem[i]))\n",
    "    #     j = i+1\n",
    "    #     while j < len(basketItem):\n",
    "    #         result.add((basketItem[i],basketItem[j]))\n",
    "    #         j+=1\n",
    "    #     i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateItemsetCount(fullItemset: dict, basketItemset):\n",
    "    \"\"\"\n",
    "    Update basket's itemset into full itemset\n",
    "    if an itemset is already in the full itemset, increase its count by 1 in full itemset\n",
    "    otherwise, add to the full itemset by creating new record of itself in full itemset with value 1\n",
    "\n",
    "    :param fullItemset: full itemset\n",
    "    :param basketItemset: basket itemset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for itemset in basketItemset:\n",
    "        if itemset in fullItemset:\n",
    "            # id = fullItemset.index(itemset)\n",
    "            fullItemset[itemset] +=1\n",
    "        else:\n",
    "            fullItemset[itemset] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLine(line:str):\n",
    "    \"\"\"\n",
    "    Convert line into list of int\n",
    "\n",
    "    :param line: basket data\n",
    "    :return: list of item's id\n",
    "    \"\"\"\n",
    "    id_str = line.split()\n",
    "    return list(map(int, id_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFileLine(line):\n",
    "    ids = convertLine(line)\n",
    "    return generateItemset(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filterResult(itemset:dict, leastSupportCount)-> dict:\n",
    "    \"\"\"\n",
    "    Filter input itemset for only accepting ones with frequent greater or equal given threshold\n",
    "    Input itemset must be sorted for correct result\n",
    "\n",
    "    :param itemset: collected itemsets and their frequent\n",
    "    :param leastSupportCount: given threshold\n",
    "    :return: itemsets that satisfy the condition\n",
    "    \"\"\"\n",
    "    result = dict()\n",
    "    for key in itemset.keys():\n",
    "        if itemset[key] < leastSupportCount: break\n",
    "        result[key] = itemset[key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLimitedPassAlgo(filePath, m, p, s, resultLogPath, random, rangeId):\n",
    "    \"\"\"\n",
    "    Modified version for run both randomize and SON\n",
    "\n",
    "    :param filePath:\n",
    "    :param m:\n",
    "    :param p: selecting ratior for randomize, 1/c for SON\n",
    "    :param s:\n",
    "    :param resultLogPath:\n",
    "    :param random: True for run randomized , False for run SON algo\n",
    "    :param rangeId: only need when run SON\n",
    "    :return: limited-pass frequent itemset\n",
    "    \"\"\"\n",
    "    \n",
    "    numberBasket = m\n",
    "\n",
    "    leastSupportCount = 0\n",
    "    intro = ''\n",
    "\n",
    "    processLineId = []\n",
    "    if random: \n",
    "        processLineId = randomSelect(numberBasket, p)\n",
    "        leastSupportCount = int(0.9*p*s*numberBasket)\n",
    "        intro = f'processing {filePath}, numberBasket={numberBasket}, numberSelecting={len(processLineId)}'\n",
    "        intro += f', leastSupportCount={leastSupportCount}'\n",
    "    else: \n",
    "        processLineId = rangeId\n",
    "        leastSupportCount = int(s*numberBasket)\n",
    "        intro = f'processing {filePath}[{processLineId[0]}:{processLineId[-1]}]'\n",
    "    fullItemset = dict()\n",
    "\n",
    "    if random: print(intro)\n",
    "\n",
    "    timeStart = datetime.now()\n",
    "    currentLine = 0\n",
    "    with open(filePath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if (currentLine in processLineId) and (len(line)>0):\n",
    "                updateItemsetCount(fullItemset, processFileLine(line))\n",
    "\n",
    "            currentLine += 1\n",
    "    # for itemset in fullItemset:\n",
    "    #     itemset.conf = itemset.count/numberBasket\n",
    "\n",
    "    frequentItemset = dict(sorted(fullItemset.items(),key=lambda item: item[1], reverse=True))\n",
    "    if random:\n",
    "        frequentItemset = filterResult(frequentItemset,leastSupportCount)\n",
    "    timeStop = datetime.now()\n",
    "\n",
    "    end = f'finish in {str(timeStop - timeStart)}'\n",
    "    if random: print(end)\n",
    "        \n",
    "    with open(resultLogPath,'w') as file:\n",
    "        file.write(f'{intro}\\n{end}\\n')\n",
    "        for key in frequentItemset.keys():\n",
    "            file.write(f'{key}->{frequentItemset[key]}\\n')\n",
    "\n",
    "    return frequentItemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDirectory(path):\n",
    "    \"\"\"\n",
    "    Build folder based on given path\n",
    "    Support nested folder, ie. './a/b/c', \n",
    "    \"\"\"\n",
    "    path = path.split(os.sep)\n",
    "    currentDir = '.'\n",
    "    for i in path:\n",
    "        currentDir += os.sep + i\n",
    "        try:\n",
    "            os.mkdir(currentDir)\n",
    "        except:\n",
    "            # this folder is existed\n",
    "            continue\n",
    "p=.05\n",
    "s=.5\n",
    "dataDir = './data/'\n",
    "resultDir = f'{dataDir}/p{p}s{s}/'\n",
    "createDirectory(resultDir)\n",
    "\n",
    "fileDataPath = []\n",
    "for a in os.listdir(dataDir):\n",
    "    if a.endswith('.dat'): fileDataPath.append(dataDir + a)\n",
    "fileDataPath = sorted(fileDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run randomised algo as requested in 1.3**\n",
    "\n",
    "Dataset for running need to be found in folder \"data\"<br/>\n",
    "Result will be saved in folder with format \"data/p{s}s{s}\", ex. \"data/p0.05s0.5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ./data/T10I4D100K.dat, numberBasket=100000, numberSelecting=5120, leastSupportCount=2250\n",
      "finish in 0:00:06.740719\n",
      "processing ./data/T40I10D100K.dat, numberBasket=100000, numberSelecting=4973, leastSupportCount=2250\n",
      "finish in 0:01:43.280514\n",
      "processing ./data/chess.dat, numberBasket=3196, numberSelecting=153, leastSupportCount=71\n",
      "finish in 0:00:00.583342\n",
      "processing ./data/connect.dat, numberBasket=67557, numberSelecting=3447, leastSupportCount=1520\n",
      "finish in 0:00:31.512795\n",
      "processing ./data/mushroom.dat, numberBasket=8124, numberSelecting=398, leastSupportCount=182\n",
      "finish in 0:00:00.295855\n",
      "processing ./data/pumsb.dat, numberBasket=49046, numberSelecting=2468, leastSupportCount=1103\n",
      "finish in 0:02:57.801484\n",
      "processing ./data/pumsb_star.dat, numberBasket=49046, numberSelecting=2393, leastSupportCount=1103\n",
      "finish in 0:00:52.036848\n"
     ]
    }
   ],
   "source": [
    "for path in fileDataPath:\n",
    "    m = sum(1 for line in open(path))\n",
    "    outputPath = path.replace('.dat','.resultv2')\\\n",
    "                        .replace(dataDir,resultDir)\n",
    "    processLimitedPassAlgo(path, m, p, s, outputPath, True, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Implement SON algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SON algorithm is implemented as follow phases:\n",
    "**Phase 1:**\n",
    "1. Divide full dataset into $c$ chunks\n",
    "    * modify RandomAlgo to take a range of line Id instead of randomly generating\n",
    "    * equivalently $p=\\frac{1}{c}$\n",
    "1. For each of data chunk:\n",
    "    * handled by a parallel 'process'\n",
    "    * each 'process' execute same process as Random Algorithm\n",
    "    * store outputs into files\n",
    "\n",
    "**Phase 2:**\n",
    "1. Collect and summarize frequent itemset from output files\n",
    "1. Sort itemsets and get most frequent itemset\n",
    "    * expected confident is $s$\n",
    "    * equivalently change to least support count $sm$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation for phase 1**\n",
    "\n",
    "I chose $multiprocessing.Pool$ for implementing paralle processing.<br/>\n",
    "$Pool$ will be initialize with a specific amount of subprocess, ie. 5 in my implementation. Those subprocess is automatically mapped with given function and given arguments set, then will be executed independently from the main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def buildDataForPool(path, m, p, s, outputPath):\n",
    "    \"\"\"\n",
    "    Build list of arguments list for pool's subprocess\n",
    "    \n",
    "    :param filePath: path to dataset\n",
    "    :param m: \n",
    "    :param p: \n",
    "    :param s: \n",
    "    :param outputPath: output path for chunk's result\n",
    "    :return: list of arguments list for pool's subprocess\n",
    "    \"\"\"\n",
    "    \n",
    "    data =[]\n",
    "    mOVerC = int(m/c)\n",
    "    maxCounter = c if m%c==0 else c+1\n",
    "\n",
    "    for counter in range(maxCounter):\n",
    "        # each item must strictly follow the argument order from\n",
    "        # processLimitedPassAlgo(filePath, m, p, s, resultLogPath, random, rangeId):\n",
    "        data.append([path, m, p, s, outputPath+str(counter), False\n",
    "                        , [x for x in range(mOVerC*counter, min(m, mOVerC*(counter+1) ))]\n",
    "                     ])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parallelProcess(processInput):\n",
    "    \"\"\"\n",
    "    Execute processLimitedPassAlgo with given argument list\n",
    "    \"\"\"\n",
    "    \n",
    "#     processLimitedPassAlgo(filePath, m, p, s, resultLogPath, random, rangeId):\n",
    "    processLimitedPassAlgo(processInput[0],processInput[1],processInput[2],processInput[3]\n",
    "                           ,processInput[4],processInput[5],processInput[6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = Pool(5)\n",
    "c = 100\n",
    "resultDir = f'{dataDir}SONc{c}s{s}/'\n",
    "createDirectory(resultDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation for phase 2:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collectItemsetFromChunks(chunkResultList):\n",
    "    \"\"\"\n",
    "    Combine full itemset result from all created results\n",
    "    \n",
    "    :param chunkResultList: list of chunk's result files, ie. chess.result.sonXX for chess.dat\n",
    "    :return: full itemset\n",
    "    \"\"\"\n",
    "    \n",
    "    result = dict()\n",
    "    for filePath in chunkResultList:\n",
    "        # print(f'executing collectItemsetFromChunks: {filePath}')\n",
    "        with open(filePath,'r') as file:\n",
    "            # skip 2 info lines\n",
    "            next(file)\n",
    "            next(file)\n",
    "\n",
    "            for line in file:\n",
    "                key, value = line.split('->')\n",
    "                value = int(value)\n",
    "                if key in result: result[key] += int(value)\n",
    "                else: result[key] = int(value)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collectItemsetFromDataset(datasetName, resultDir):\n",
    "    \"\"\"\n",
    "    SON phase 2's functionalities\n",
    "    \"\"\"\n",
    "    \n",
    "    timeStart = datetime.now()\n",
    "    m = dataLineCounts[f'{dataDir}{datasetName}.dat']\n",
    "    chunkResult = datasetName + '.result.son'\n",
    "    \n",
    "    # scan resultDir for related chunk result\n",
    "    chunkResultList = [resultDir + x for x in os.listdir(resultDir) if chunkResult in x]\n",
    "\n",
    "    \n",
    "    fullItemset = collectItemsetFromChunks(chunkResultList)\n",
    "    fullItemset = dict(sorted(fullItemset.items(),key=lambda item: item[1], reverse=True))\n",
    "    timeStop = datetime.now()\n",
    "    \n",
    "    print(f'finish collect from {chunkResult} in {str(timeStop - timeStart)}')\n",
    "    \n",
    "    with open(f'{resultDir}{datasetName}.final','w') as file:\n",
    "        for key in fullItemset.keys():\n",
    "            if fullItemset[key] < s*m: break\n",
    "            file.write(f'{key}->{fullItemset[key]}\\n')\n",
    "    # return fullItemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run SON algorithm as request in 1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finsih processing ./data/T10I4D100K.dat in 0:01:12.780084\n",
      "finsih processing ./data/T40I10D100K.dat in 0:15:20.015748\n",
      "finsih processing ./data/chess.dat in 0:00:06.403641\n",
      "finsih processing ./data/connect.dat in 0:04:33.407238\n",
      "finsih processing ./data/mushroom.dat in 0:00:03.636486\n",
      "finsih processing ./data/pumsb.dat in 0:23:17.710465\n",
      "finsih processing ./data/pumsb_star.dat in 0:07:42.315305\n",
      "CPU times: user 195 ms, sys: 24.8 ms, total: 219 ms\n",
      "Wall time: 52min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# run SON's phase 1\n",
    "\n",
    "for path in fileDataPath:\n",
    "    timeStart = datetime.now()\n",
    "    m = dataLineCounts[path]\n",
    "    outputPath = path.replace('.dat','.result.son')\\\n",
    "                        .replace(dataDir,resultDir)\n",
    "\n",
    "    parallelData = buildDataForPool(path, m, 1/c, s, outputPath)\n",
    "    pool.map(parallelProcess, parallelData)\n",
    "    timeStop = datetime.now()\n",
    "    print(f'finsih processing {path} in {str(timeStop-timeStart)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish collect from T10I4D100K.result.son in 0:00:26.140894\n",
      "finish collect from T40I10D100K.result.son in 0:17:00.326199\n",
      "finish collect from chess.result.son in 0:00:01.601326\n",
      "finish collect from connect.result.son in 0:00:07.835119\n",
      "finish collect from mushroom.result.son in 0:00:01.119917\n",
      "finish collect from pumsb.result.son in 0:07:01.125634\n",
      "finish collect from pumsb_star.result.son in 0:03:28.021837\n",
      "CPU times: user 27min 28s, sys: 34.8 s, total: 28min 3s\n",
      "Wall time: 28min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# run SON's phase 2\n",
    "\n",
    "for filename in fileDataNamesOnly:\n",
    "    collectItemsetFromDataset(filename,resultDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**1.3. Report the outcomes**\n",
    "\n",
    "**a3e13_result.zip** containts the outcomes of procssing following datasets:\n",
    "* T10I4D100K.dat\n",
    "* T40I10D100K.dat\n",
    "* chess.dat\n",
    "* connect.dat\n",
    "* mushroom.dat\n",
    "* pumsb.dat\n",
    "* pumsb_star.dat\n",
    "\n",
    "Folder includes:\n",
    "* folder **SONc100s0.5** containts only final results of SON's algo running with 100 chunks and s=0.5\n",
    "* folder **p0.05s0.5** containts result of randomised algo running with p=0.05 and s=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. Experiment with different sample sizes in the simple randomized algo-rithm such as 1, 2, 5, 10% and compare your results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ./data/T10I4D100K.dat, numberBasket=100000, numberSelecting=998, leastSupportCount=450\n",
      "finish in 0:00:01.349564\n",
      "processing ./data/T40I10D100K.dat, numberBasket=100000, numberSelecting=994, leastSupportCount=450\n",
      "finish in 0:00:13.346378\n",
      "processing ./data/chess.dat, numberBasket=3196, numberSelecting=31, leastSupportCount=14\n",
      "finish in 0:00:00.139196\n",
      "processing ./data/connect.dat, numberBasket=67557, numberSelecting=712, leastSupportCount=304\n",
      "finish in 0:00:05.447991\n",
      "processing ./data/mushroom.dat, numberBasket=8124, numberSelecting=105, leastSupportCount=36\n",
      "finish in 0:00:00.094768\n",
      "processing ./data/pumsb.dat, numberBasket=49046, numberSelecting=499, leastSupportCount=220\n",
      "finish in 0:00:31.899542\n",
      "processing ./data/pumsb_star.dat, numberBasket=49046, numberSelecting=452, leastSupportCount=220\n",
      "finish in 0:00:09.426814\n",
      "\n",
      "processing ./data/T10I4D100K.dat, numberBasket=100000, numberSelecting=1988, leastSupportCount=900\n",
      "finish in 0:00:02.639877\n",
      "processing ./data/T40I10D100K.dat, numberBasket=100000, numberSelecting=2006, leastSupportCount=900\n",
      "finish in 0:00:30.794645\n",
      "processing ./data/chess.dat, numberBasket=3196, numberSelecting=66, leastSupportCount=28\n",
      "finish in 0:00:00.261655\n",
      "processing ./data/connect.dat, numberBasket=67557, numberSelecting=1351, leastSupportCount=608\n",
      "finish in 0:00:12.225863\n",
      "processing ./data/mushroom.dat, numberBasket=8124, numberSelecting=156, leastSupportCount=73\n",
      "finish in 0:00:00.146941\n",
      "processing ./data/pumsb.dat, numberBasket=49046, numberSelecting=1004, leastSupportCount=441\n",
      "finish in 0:01:07.994776\n",
      "processing ./data/pumsb_star.dat, numberBasket=49046, numberSelecting=959, leastSupportCount=441\n",
      "finish in 0:00:20.473737\n",
      "\n",
      "processing ./data/T10I4D100K.dat, numberBasket=100000, numberSelecting=10098, leastSupportCount=4500\n",
      "finish in 0:00:16.604049\n",
      "processing ./data/T40I10D100K.dat, numberBasket=100000, numberSelecting=10063, leastSupportCount=4500\n",
      "finish in 0:02:58.009966\n",
      "processing ./data/chess.dat, numberBasket=3196, numberSelecting=310, leastSupportCount=143\n",
      "finish in 0:00:02.502688\n",
      "processing ./data/connect.dat, numberBasket=67557, numberSelecting=6711, leastSupportCount=3040\n",
      "finish in 0:01:24.959890\n",
      "processing ./data/mushroom.dat, numberBasket=8124, numberSelecting=808, leastSupportCount=365\n",
      "finish in 0:00:00.610134\n",
      "processing ./data/pumsb.dat, numberBasket=49046, numberSelecting=5080, leastSupportCount=2207\n",
      "finish in 0:06:57.451761\n",
      "processing ./data/pumsb_star.dat, numberBasket=49046, numberSelecting=4842, leastSupportCount=2207\n",
      "finish in 0:01:53.475391\n",
      "\n",
      "processing ./data/T10I4D100K.dat, numberBasket=100000, numberSelecting=19930, leastSupportCount=9000\n",
      "finish in 0:00:27.323365\n",
      "processing ./data/T40I10D100K.dat, numberBasket=100000, numberSelecting=20109, leastSupportCount=9000\n",
      "finish in 0:06:20.363147\n",
      "processing ./data/chess.dat, numberBasket=3196, numberSelecting=625, leastSupportCount=287\n",
      "finish in 0:00:02.419319\n",
      "processing ./data/connect.dat, numberBasket=67557, numberSelecting=13537, leastSupportCount=6080\n",
      "finish in 0:02:42.226214\n",
      "processing ./data/mushroom.dat, numberBasket=8124, numberSelecting=1662, leastSupportCount=731\n",
      "finish in 0:00:01.504648\n",
      "processing ./data/pumsb.dat, numberBasket=49046, numberSelecting=9728, leastSupportCount=4414\n",
      "finish in 0:13:11.564482\n",
      "processing ./data/pumsb_star.dat, numberBasket=49046, numberSelecting=9856, leastSupportCount=4414\n",
      "finish in 0:04:12.013575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_list = [.01,.02,.1,.2]\n",
    "\n",
    "for p in p_list:\n",
    "    s=.5\n",
    "    dataDir = './data/'\n",
    "    resultDir = f'{dataDir}/p{p}s{s}/'\n",
    "    createDirectory(resultDir)\n",
    "    \n",
    "    for path in fileDataPath:\n",
    "        m = sum(1 for line in open(path))\n",
    "        outputPath = path.replace('.dat','.resultv2')\\\n",
    "                            .replace(dataDir,resultDir)\n",
    "        processLimitedPassAlgo(path, m, p, s, outputPath, True, [])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T10I4D100K.dat and T40I10D100K.dat**\n",
    "\n",
    "Randomised algo's results and SON algo's result both show empty itemset.\n",
    "We can conclude that itemsets in those dataset are fregmented, therefore they dont have enough repeatation to achieve expected confident $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chess.dat's top 5 frequent item**\n",
    "\n",
    "| p=1% s=0.5     | p=2% s=0.5  | p=5% s=0.5  | p=10% s=0.5 | p=20% s=0.5 | SON c=100 s=0.5 |\n",
    "| -------------- | ----------- |-------------|-------------|-------------|-------------|\n",
    "|(48, 58)->31    |(58,)->66       |(52, 58)->153    |(58,)->310   |(58,)->625   |(58,)->3130\n",
    "|(40, 48, 58)->31|(60,)->66       |(58,)->153       |(52, 58)->309|(29,)->624   |(60,)->3127\n",
    "|(52, 60)->31    |(58, 60)->66    |(52,)->153       |(52,)->309   |(52, 58)->624|(58, 60)->3126\n",
    "|(40, 48, 60)->31|(29, 58, 60)->65|(40,)->152       |(29,)->308   |(29, 58)->624|(52,)->3120\n",
    "|(40,)->31       |(52, 60)->65    |(40, 52, 58)->152|(29, 58)->308|(52,)->624   |(52, 58)->3119\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider as SON has the most accurate (process 100% dataset instead of picking random parts)\n",
    "* p=20%'s result containts 3 correct frequent itemset\n",
    "* p=10%'s result containts 2 correct frequent itemset\n",
    "* p=5%'s result containts 3 correct frequent itemset\n",
    "* p=2%'s result containts 1 correct frequent itemset\n",
    "* p=1%'s result containts 0 correct frequent itemset\n",
    "\n",
    "The incorrect itemset appear in other results are called *false negative* <br>\n",
    "The higher value of $p$, the lesser false negative we get. \n",
    "\n",
    "The frequent of itemset seems to be more accurate with higher $p$. \n",
    "* SON result indicates singleton (58) repeats 3130 times \n",
    "* p=20%'s indicates singleton (58) repeats 625 times -> 625*5 = 3125 times over the whole dataset: extremely closed to SON's result\n",
    "* p=10%'s indicates singleton (58) repeats 310 times -> 310*10 = 3100 times over the whole dataset: closed\n",
    "* p=5%'s indicates singleton (58) repeats 153 times -> 153*20 = 3060 times over the whole dataset: the differences is ~100\n",
    "* p=2%'s indicates singleton (58) repeats 66 times  -> 66*50=3300 times over the whole dataset: the differences is ~170\n",
    "* p=1%'s indicates singleton (58) repeats 31 times  -> 31*100=3100 times over the whole dataset: really closed, but (58) is only the 19th among most frequent itemset, while it actually the 1st one in final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar analyze could be done based other dataset's results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
